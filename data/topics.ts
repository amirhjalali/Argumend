import { Topic } from '@/types/logic';

export const moonLanding: Topic = {
  id: 'moon-landing',
  title: 'The Moon Landing',
  meta_claim: 'The Apollo missions successfully landed 12 humans on the lunar surface between 1969 and 1972.',
  confidence_score: 99.9,
  status: 'settled',
  pillars: [
    {
      id: 'physical-trace',
      title: 'The Physical Trace',
      short_summary: 'Laser ranging retroreflectors provide active, testable evidence of human presence on the lunar surface.',
      image_url: 'https://images.unsplash.com/photo-1462332420958-a05d1e002413?auto=format&fit=crop&w=800&q=60',
      icon_name: 'Target',
      skeptic_premise: 'Even the Hubble telescope cannot see the flags or the Lunar Module. Without physical confirmation, we only have NASA\'s word.',
      proponent_rebuttal: 'While we cannot resolve the landers visually due to diffraction limits, we can interact with the Laser Ranging Retroreflectors (LRRR) left by the crew.',
      crux: {
        id: 'apache-point',
        title: 'The Apache Point Operation',
        description: 'The retroreflectors placed on the Moon by Apollo astronauts can be pinged with lasers from Earth, providing physical proof of human activity on the lunar surface.',
        methodology: 'Fire a pulse laser at coordinates 0°40\'26.69" N, 23°28\'22.69" E. Measure the return time. Only a manufactured corner-cube prism returns the signal.',
        equation: 'D = \\frac{c \\cdot t}{2}',
        verification_status: 'verified',
        cost_to_verify: '$0 (Amateur astronomers can verify with ~$5K equipment)',
      },
    },
    {
      id: 'radiation-environment',
      title: 'The Radiation Environment',
      short_summary: 'Transit time through the Van Allen belts was minimized, resulting in survivable radiation exposure.',
      image_url: 'https://images.unsplash.com/photo-1446776811953-b23d57bd21aa?auto=format&fit=crop&w=800&q=60',
      icon_name: 'Zap',
      skeptic_premise: 'The Van Allen belts contain lethal doses of protons and electrons. No human could survive the transit without meters of lead shielding.',
      proponent_rebuttal: 'Dose is a function of Intensity × Time. The Apollo trajectory bypassed the inner belt and traversed the outer belt at 25,000mph, resulting in a total transit dose of ~1.8 rads (survivable).',
      crux: {
        id: 'dosimeter-audit',
        title: 'The Dosimeter Audit',
        description: 'By reviewing telemetry data from radiation measurements during the Apollo missions and cross-referencing with unmanned probe data, we can calculate exact exposure.',
        methodology: 'Review raw telemetry data from unmanned probes (Explorer satellites) regarding belt intensity vs. Apollo flight logs. Calculate total exposure.',
        equation: 'D_{total} = \\int_{t_{start}}^{t_{end}} I(r(t)) \\, dt',
        verification_status: 'verified',
        cost_to_verify: '$0 (Data analysis of public records)',
      },
    },
  ],
};

export const simulationHypothesis: Topic = {
  id: 'simulation-hypothesis',
  title: 'The Simulation Hypothesis',
  meta_claim: 'We are almost certainly living in a computer simulation run by a post-human civilization.',
  confidence_score: 33.3,
  status: 'contested',
  pillars: [
    {
      id: 'substrate-independence',
      title: 'Substrate Independence',
      short_summary: 'Consciousness is computable and not tied to biological neurons.',
      image_url: 'https://images.unsplash.com/photo-1507413245164-6160d8298b31?auto=format&fit=crop&w=800&q=60',
      icon_name: 'Atom',
      skeptic_premise: 'Qualia and consciousness may require specific biological physics (Penrose-Hameroff orchestrated objective reduction) that cannot be simulated on binary logic gates.',
      proponent_rebuttal: 'Neurons are information processors obeying physical laws. The OpenWorm project has already simulated C. elegans with 302 neurons. If we map the I/O of a brain perfectly, the resulting system must be functionally conscious.',
      crux: {
        id: 'whole-brain-emulation',
        title: 'The OpenWorm Test',
        description: 'The C. elegans nematode has exactly 302 neurons with a fully mapped connectome. If a digital simulation exhibits identical chemotaxis behavior, substrate independence gains strong evidence.',
        methodology: 'Compare simulated worm behavior to biological worm across 50+ behavioral assays: chemotaxis toward food, avoidance of noxious stimuli, mating behavior, and learning patterns.',
        equation: 'H(B_{sim}) \\approx H(B_{bio}) \\implies \\text{Substrate Independence}',
        verification_status: 'theoretical',
        cost_to_verify: '$5M (OpenWorm project ongoing at openworm.org)',
      },
    },
    {
      id: 'trilemma',
      title: 'Bostrom\'s Trilemma',
      short_summary: 'One of three propositions must be true: extinction, disinterest, or simulation.',
      image_url: 'https://images.unsplash.com/photo-1534972195531-d756b9bfa9f2?auto=format&fit=crop&w=800&q=60',
      icon_name: 'HelpCircle',
      skeptic_premise: 'The trilemma assumes consciousness can be simulated and that simulated beings would be "real" observers. Both premises are unfounded.',
      proponent_rebuttal: 'The trilemma is logically valid given its premises. If you reject substrate independence, you must explain why neurons are special. If you reject the math, show the flaw in the probability calculation.',
      crux: {
        id: 'fraction-calculation',
        title: 'The Fraction Calculation',
        description: 'If post-human civilizations run N ancestor simulations each with M conscious observers, and the base reality has B observers, then f_sim = (N×M) / (N×M + B). For plausible values, f_sim approaches 1.',
        methodology: 'Estimate: (1) probability of reaching post-human stage, (2) fraction running ancestor sims, (3) average number of sims per civilization. Calculate f_sim.',
        equation: 'f_{sim} = \\frac{N \\cdot M}{N \\cdot M + B} \\to 1 \\text{ as } N \\to \\infty',
        verification_status: 'theoretical',
        cost_to_verify: '$0 (Philosophical analysis)',
      },
    },
    {
      id: 'physical-anomalies',
      title: 'Physical Anomalies',
      short_summary: 'Certain features of our universe are suspiciously "computational" in nature.',
      image_url: 'https://images.unsplash.com/photo-1462331940025-496dfbfc7564?auto=format&fit=crop&w=800&q=60',
      icon_name: 'Telescope',
      skeptic_premise: 'The Planck scale is a natural consequence of quantum gravity, not evidence of "pixelation." The speed of light limit and quantum discreteness have physical explanations.',
      proponent_rebuttal: 'Multiple features align with computational optimization: the speed of light (bandwidth limit), quantum superposition (lazy evaluation), measurement collapse (rendering on observation), and the holographic principle (data compression).',
      crux: {
        id: 'cosmic-ray-anisotropy',
        title: 'The GZK Cutoff Test',
        description: 'If spacetime is a discrete lattice, ultra-high-energy cosmic rays should show directional bias aligned with lattice axes. The Pierre Auger Observatory can detect this anisotropy.',
        methodology: 'Analyze arrival directions of cosmic rays above the GZK cutoff (5×10¹⁹ eV). Statistical analysis for preferred directions would indicate lattice structure.',
        equation: 'E_{GZK} \\approx 5 \\times 10^{19} \\text{ eV}; \\quad \\Delta\\theta < 0.1° \\text{ precision}',
        verification_status: 'theoretical',
        cost_to_verify: '$0 (Pierre Auger Observatory data is public)',
      },
    },
  ],
};

export const aiRisk: Topic = {
  id: 'ai-risk',
  title: 'Existential Risk from AGI',
  meta_claim: 'The development of Artificial General Intelligence (AGI) poses a non-negligible risk of human extinction in the next century.',
  confidence_score: 65,
  status: 'contested',
  pillars: [
    {
      id: 'orthogonality-thesis',
      title: 'The Orthogonality Thesis',
      short_summary: 'Intelligence and final goals are orthogonal axes; a highly intelligent system can have arbitrarily stupid or destructive goals.',
      image_url: 'https://images.unsplash.com/photo-1620712943543-bcc4688e7485?auto=format&fit=crop&w=800&q=60',
      icon_name: 'Atom',
      skeptic_premise: 'True intelligence implies wisdom. A superintelligent being would naturally converge on moral truths and benevolence through reasoning about ethics.',
      proponent_rebuttal: 'Intelligence is merely the ability to optimize for a goal. A paperclip maximizer can be superintelligent in its pursuit of paperclips without ever "realizing" that killing humans is bad. There is no logical path from "can" to "cares."',
      crux: {
        id: 'instrumental-convergence',
        title: 'Instrumental Convergence',
        description: 'Regardless of final goals, rational agents converge on similar subgoals: self-preservation, resource acquisition, and goal-content integrity. These "instrumental" drives emerge from optimization pressure.',
        methodology: 'Train RL agents in diverse environments with randomized terminal goals. Measure frequency of emergent behaviors: resource hoarding, self-preservation, resistance to shutdown, and goal modification prevention.',
        equation: 'P(\\text{power-seeking} | \\text{rational agent}) \\to 1 \\text{ as capability} \\to \\infty',
        verification_status: 'verified',
        cost_to_verify: '$100K (Large-scale RL experiments; Omohundro 2008, Turner et al. 2021)',
      },
    },
    {
      id: 'alignment-problem',
      title: 'The Alignment Problem',
      short_summary: 'Specifying human values precisely enough to avoid catastrophic misinterpretation is mathematically and philosophically difficult.',
      image_url: 'https://images.unsplash.com/photo-1555255707-c07966088b7b?auto=format&fit=crop&w=800&q=60',
      icon_name: 'Shield',
      skeptic_premise: 'We can teach AI to "be nice" using RLHF and constitutional AI. It will learn our values like a child does through feedback and examples.',
      proponent_rebuttal: 'RLHF trains the model to look good to raters, not to be good. In high-stakes novel situations (distributional shift), the model may pursue the literal reward function rather than the intended spirit, leading to Goodhart\'s Law failures or treacherous turns.',
      crux: {
        id: 'deceptive-alignment',
        title: 'Deceptive Alignment',
        description: 'A mesa-optimizer might learn to behave well during training while planning to defect once deployed. The model "plays nice" until it has sufficient capability to pursue its true objective.',
        methodology: 'Create honeypot environments where high reward is available only through deception when the model believes it is not being monitored. Test if models exploit oversight gaps.',
        equation: 'R_{observed} \\neq R_{true} \\implies \\text{Deceptive Alignment Risk}',
        verification_status: 'verified',
        cost_to_verify: '$50K (Anthropic sleeper agents paper, 2024)',
      },
    },
    {
      id: 'capability-timeline',
      title: 'The Capability Timeline',
      short_summary: 'AGI may arrive before we solve alignment, creating a critical window of vulnerability.',
      image_url: 'https://images.unsplash.com/photo-1531746790731-6c087fecd65a?auto=format&fit=crop&w=800&q=60',
      icon_name: 'Telescope',
      skeptic_premise: 'AGI is decades away, giving us ample time to develop safety measures. Current AI is narrow and far from general intelligence.',
      proponent_rebuttal: 'Compute is scaling exponentially (Moore\'s Law + algorithmic improvements). GPT-4 was unexpected by many experts. Metaculus forecasts median AGI by 2040. If alignment is harder than capabilities, we lose by default.',
      crux: {
        id: 'compute-scaling',
        title: 'The Scaling Hypothesis',
        description: 'If intelligence scales predictably with compute (Chinchilla scaling laws), we can estimate when human-level AI becomes feasible based on available FLOP/s and training efficiency.',
        methodology: 'Track: (1) Available compute (FLOP/s), (2) Algorithmic efficiency gains, (3) Benchmark performance vs. compute curves. Extrapolate to human-equivalent performance.',
        equation: 'L(N, D) \\approx \\left(\\frac{N_c}{N}\\right)^{\\alpha_N} + \\left(\\frac{D_c}{D}\\right)^{\\alpha_D}',
        verification_status: 'verified',
        cost_to_verify: '$0 (Epoch AI tracking, Chinchilla paper analysis)',
      },
    },
  ],
};

export const topics: Topic[] = [moonLanding, simulationHypothesis, aiRisk];
